{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LkksZfmSHTXp"
      ],
      "name": "BA820_M4_Q2_Drishti_Chulani.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BackBencher2424/BA820_Team_14_Project/blob/main/BA820_M4_Q2_Drishti_Chulani.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**BA820 – Project M4**\n",
        "**Project Title:** *Code Trends, Quantified: Mapping the Programming Language Ecosystem* <br>\n",
        "***Section:** B1* <br>\n",
        "***Team:** 14* <br>\n",
        "**Team Member:** Drishti Chulani <br><br>\n",
        "\n",
        "**Research Question:**\n",
        "\n",
        "\n",
        "***Link to Proposal Notebook (BA820_Team_14_Project_Proposal_Notebook.ipynb):** [EDA Notebook](https://colab.research.google.com/drive/1irElxdNYp_Hh08p4MeOGdafvt_d1KsT7?usp=sharing)* <br>\n",
        "\n",
        "**Link to M2 Notebook:**\n",
        "\n",
        "**Link to M3 Notebook:**\n",
        "\n",
        "**Link to Colab Notebook:** [Research Question 2 Notebook](https://colab.research.google.com/gist/DrishtiChulani/2a94bebfc93ffe65ab51a63abc7ee2c3/ba820_m2_q2_drishti_chulani.ipynb)* <br>\n",
        "\n",
        "**Dataset:** [Programming Languages Dataset Link](https://github.com/rfordatascience/tidytuesday/tree/main/data/2023/2023-03-21)*<br>\n",
        "\n",
        "**Link to Github Repo:** [Github Repo Link](https://github.com/BackBencher2424/BA820_Team_14_Project)*\n"
      ],
      "metadata": {
        "id": "mrp7EQK_vu0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Table of Contents**\n",
        "---\n",
        "\n",
        "**1. Project Framing & M4 Refinement Goals**\n",
        "\n",
        "**2. Data Pipeline & \"Active Market\" Filtering**\n",
        "\n",
        "**3. Refined EDA: The Case for Deep Learning**\n",
        "\n",
        "**4. Baseline Model (M3 Recap)**\n",
        "\n",
        "**5. M4 Method Upgrade: Autoencoder Feature Extraction**\n",
        "\n",
        "**6. Advanced Clustering & UMAP Visualization**\n",
        "\n",
        "**7. Business Insights: Identifying \"Archetype Shifts\"**"
      ],
      "metadata": {
        "id": "kM56rAvgJYMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Project Framing & M4 Refinement Goals**\n",
        "\n",
        "**The M3 Baseline:** In the previous milestones, I segmented the programming language ecosystem into four Market Archetypes — Titans, Speculative/Hype, Silent Workhorses, and the Long-Tail — using K-Means clustering and linear PCA on log-transformed metrics like GitHub stars, job postings, and Wikipedia views.\n",
        "\n",
        "**The Limitation:** That foundation held up well, but surface metrics have real weaknesses. They tend to be skewed, multicollinear, and ill-suited for capturing the non-linear relationship between a language's social hype and its actual industrial utility. There's also a dataset composition problem: roughly 90% of the languages in our dataset are dormant or \"ghost\" languages, and including them pulls cluster centroids in misleading directions — obscuring the market behaviors we actually care about.\n",
        "\n",
        "**The M4 Refinement (Method Upgrade & Experimental Rigor):**\n",
        "M4 addresses these gaps through three upgrades to get more actionable insights for technical stakeholders:\n",
        "\n",
        "* **\"Active Market\" Filtering:** I applied a strict filter to remove dormant languages before any modeling begins, so the unsupervised models are trained exclusively on the languages that are actually alive in the ecosystem.\n",
        "* **Deep Representation Learning (Autoencoder):** Rather than relying on linear scaling, I used a deep learning architecture to compress our features into a neural network's bottleneck layer — extracting the non-linear \"latent space\" that captures the fundamental DNA of each language in a way PCA simply can't.\n",
        "* **Advanced Manifold Visualization (UMAP):** I applied UMAP to this latent space to visualize cluster boundaries that reflect genuine structural relationships rather than linear projections.\n",
        "\n",
        "**The Business Goal:** By clustering on a language's deep technical DNA rather than surface-level buzz, we aim to identify **\"Archetype Shifts\"**; hidden Titans working under the radar, or hype-driven languages masking weak fundamentals. The goal is to give CTOs and investors a clearer, more honest way to quantify adoption risk."
      ],
      "metadata": {
        "id": "1k6HcjpVL7cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Data Pipeline & \"Active Market\" Filtering**"
      ],
      "metadata": {
        "id": "FW58kH29wDTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Importing Libraries***"
      ],
      "metadata": {
        "id": "tIzwRD6SwJ_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn\n",
        "!pip install --upgrade patsy statsmodels"
      ],
      "metadata": {
        "id": "oZLJZk1g0OFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import umap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "C-k2Qcd-fvNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Loading Dataset***"
      ],
      "metadata": {
        "id": "oyTxzqtUwa_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/languages.csv\"\n",
        "df = pd.read_csv(path) # Loads the primary dataset to the runtime"
      ],
      "metadata": {
        "id": "kqefT3kuwgNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1. Preprocessing**"
      ],
      "metadata": {
        "id": "COGmh7vKMxSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Standardizing and Exploring the Dataset***"
      ],
      "metadata": {
        "id": "dSUQkLO5yX-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = [c.strip().lower() for c in df.columns] # Standardizes the columns, strip() —->> removes any trailing or leading spaces, lower() -->> makes all the column names lowercase"
      ],
      "metadata": {
        "id": "sj6YB4HRylLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape (rows, cols):\", df.shape) #Gives number of Rows and Columns"
      ],
      "metadata": {
        "id": "oRE8C3U9dPf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "Im9H8sRdHj-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Calculating Missing & Duplicate Values***\n"
      ],
      "metadata": {
        "id": "Zlr_KK_Jk21I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can't drop any columns based on null values because programming languages have been updated and invented quite a lot and most of them have either some or the other missing information, so we work with the missing values.\n"
      ],
      "metadata": {
        "id": "aFYdgLSjLczp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values summary gives columns missing_count and missing_pct\n",
        "\n",
        "missing = (df.isna().sum()\n",
        "           .to_frame(\"missing_count\")\n",
        "           .assign(missing_pct=lambda x: (x[\"missing_count\"] / len(df) * 100).round(2))\n",
        "           .sort_values(\"missing_pct\", ascending=False))\n",
        "\n",
        "missing.head(20)  # top 20 columns with most missingness"
      ],
      "metadata": {
        "id": "JxHATxtbjHrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Feature Selection & Type Conversion***"
      ],
      "metadata": {
        "id": "oGGJf3S4kUJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['github_repo_stars', 'number_of_jobs', 'number_of_users', 'wikipedia_daily_page_views']\n",
        "df_eda = df.copy()\n",
        "\n",
        "# Coerce to numeric and fill NaNs with 0\n",
        "for feature in features:\n",
        "    df_eda[feature] = pd.to_numeric(df_eda[feature], errors='coerce')\n",
        "df_eda[features] = df_eda[features].fillna(0)"
      ],
      "metadata": {
        "id": "P_iEX3wBlofF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Numeric Summary Statistics***"
      ],
      "metadata": {
        "id": "fB2MPWqhpEhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic summary statistics for selected numeric columns, because these are most relevant columns\n",
        "cols_to_summarize = [\n",
        "    \"github_repo_stars\",\"wikipedia_daily_page_views\",\n",
        "    \"number_of_users\", \"number_of_jobs\"\n",
        "]\n",
        "cols_to_summarize = [c for c in cols_to_summarize if c in df_eda.columns]\n",
        "\n",
        "df_eda[cols_to_summarize].describe().T"
      ],
      "metadata": {
        "id": "dk4wbLGjor15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Few Key Findings***"
      ],
      "metadata": {
        "id": "s-nrYJZUKryz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***What are the top 10 programming languages that have the most number of jobs?***"
      ],
      "metadata": {
        "id": "LkksZfmSHTXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 Languages by Jobs\n",
        "print(\"Top 10 by Jobs:\\n\", df_eda[['title', 'number_of_jobs']].nlargest(10, 'number_of_jobs'))"
      ],
      "metadata": {
        "id": "Uusn7S5EKC9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***What are the top 10 programming languages that have the most number of users?***"
      ],
      "metadata": {
        "id": "HqDYuMm0J9KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 by Users\n",
        "print(\"\\nTop 10 by Users:\\n\", df_eda[['title', 'number_of_users']].nlargest(10, 'number_of_users'))"
      ],
      "metadata": {
        "id": "QWHJdgs0HS6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have used these 2 tables as evidence for our clusters"
      ],
      "metadata": {
        "id": "Mny_bijUt2cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**M4 \"Active Market\" Filter**"
      ],
      "metadata": {
        "id": "qpR15LOQqUeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_active = (df_eda['number_of_jobs'] > 0) | (df_eda['github_repo_stars'] > 0)\n",
        "df_active = df_eda[mask_active].reset_index(drop=True)\n",
        "\n",
        "print(f\"Original Dataset: {len(df_eda)} languages\")\n",
        "print(f\"Active Market Subset: {len(df_active)} languages\")\n",
        "print(f\"Noise Removed: {len(df_eda) - len(df_active)} inactive/dead languages\")\n",
        "\n",
        "# Transformation & Scaling (The Preprocessing Pipeline)\n",
        "# Apply Log10(x+1) to handle extreme skewness in the active dataset\n",
        "df_active_log = np.log10(df_active[features].clip(lower=0) + 1)\n",
        "\n",
        "# Standardize for clustering and neural network input\n",
        "scaler = StandardScaler()\n",
        "df_active_scaled = scaler.fit_transform(df_active_log)"
      ],
      "metadata": {
        "id": "uuQhyQNNrluL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Why this methodology**\n",
        "\n",
        "In Milestone 2 and 3, our clustering analysis included the entire dataset of over 4,000 languages. However, a significant portion of these entries (~75%) represent \"Ghost\" or \"Historical\" languages with zero job postings and zero GitHub activity.\n",
        "\n",
        "What was filtered?\n",
        "\n",
        "**Centroid Distortion**: Including thousands of inactive languages with zero-value signals artificially pulls the cluster centroids toward the origin. This makes it mathematically difficult to distinguish between \"Niche\" languages and \"Speculative\" languages that are just starting to gain traction.\n",
        "\n",
        "**Model Sparsity**: High-dimensionality models like Autoencoders perform best when the input data represents a viable signal rather than sparse \"noise\".\n",
        "\n",
        "**Business Relevance**: For a CTO or developer, an archetype model is most valuable when it compares technologies that are actually competing in the current market.\n",
        "\n",
        "####**M2 refinements**\n",
        "\n",
        "The logarithmic transformation ($log_{10}(x+1)$) was a necessary analytical choice to handle extreme scale differences and data noise. While the majority of languages show zero activity, Famous well known languages like SQL have over 7.1 million users, a disparity that would make standard clustering impossible. This transformation \"squashes\" these outliers, allowing the model to detect meaningful patterns across the entire dataset.\n",
        "\n",
        "Additionally, we encountered \"noise\" where wikipedia_daily_page_views had values of -1.0. Since log functions are mathematically undefined for values $\\leq 0$, we clipped these negatives to 0 before transforming. This prevented errors like negative infinity while maintaining the integrity of our four \"Market Archetypes\"."
      ],
      "metadata": {
        "id": "kXeM62W05MHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Refined EDA**\n",
        "\n",
        "In this section, we examine the statistical properties of the \"Active Market\" subset. Our goal is to justify the transition from Milestone 3's linear baseline to Milestone 4's deep representation learning.\n",
        "\n",
        "#### **3.1 Distribution and Skewness**\n",
        "Even after filtering for active languages, the data remains heavily skewed. We observe that while the majority of active languages have modest engagement, a select few \"Titans\" command millions of users and jobs. This necessitates the use of a neural network that can learn to \"weigh\" these extremes without being overwhelmed by them.\n",
        "\n",
        "#### **3.2 Multicollinearity: Redundant Signals**\n",
        "Our community metrics (Stars, Wiki Views, and Users) show high correlation. In a standard clustering model, this redundancy can lead to \"double-counting\" popularity while ignoring subtle differences in industrial utility. An **Autoencoder** is specifically designed to compress these correlated signals into a clean, lower-dimensional latent space, effectively \"denoising\" the market archetypes."
      ],
      "metadata": {
        "id": "VtWRvm7Xay0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Condensed Distribution Plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: GitHub Stars\n",
        "sns.histplot(df_active_log['github_repo_stars'], bins=30, kde=True, ax=axes[0], color='skyblue')\n",
        "axes[0].set_title(\"Log-Scaled GitHub Stars\")\n",
        "\n",
        "# Plot 2: Wikipedia Views\n",
        "sns.histplot(df_active_log['wikipedia_daily_page_views'], bins=30, kde=True, ax=axes[1], color='salmon')\n",
        "axes[1].set_title(\"Log-Scaled Wikipedia Views\")\n",
        "\n",
        "# Plot 3: Number of Jobs\n",
        "sns.histplot(df_active_log['number_of_jobs'], bins=30, kde=True, ax=axes[2], color='lightgreen')\n",
        "axes[2].set_title(\"Log-Scaled Number of Jobs\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Correlation Heatmap (Justifying the Bottleneck)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df_active_log.corr(), annot=True, cmap='mako', fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Heatmap: Identifying Redundancy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OD8s-r4WbPTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Baseline Model (M3 Recap)**\n",
        "\n",
        "Before implementing the deep learning approach, I used Milestone 3 methodology as the baseline/\n",
        "\n",
        "**The Baseline Approach:**\n",
        "* **Algorithm:** K-Means Clustering ($k=4$).\n",
        "* **Representation:** Raw log-scaled features (`df_active_scaled`).\n",
        "* **Visualization:** Manifold projection via **UMAP** (Uniform Manifold Approximation and Projection).\n",
        "\n",
        "**Purpose:** By running UMAP on the baseline scaled data, we can visualize the existing \"Market Archetypes.\" This will allow us to document the limitations of linear scaling and hard clustering (such as overlapping boundaries or \"noisy\" segments) which we aim to solve in Section 6."
      ],
      "metadata": {
        "id": "jDSYzNUmbpV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. using baseline K-Means on the active languages\n",
        "# Using k=4 as validated in M2 and M3 via Elbow and Silhouette scores\n",
        "kmeans_base = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "df_active['baseline_cluster'] = kmeans_base.fit_predict(df_active_scaled)\n",
        "\n",
        "# 2. Baseline UMAP for visualization\n",
        "# This projects our 4D data into 2D to visualize the \"surface\" cluster separation [cite: 70]\n",
        "reducer_base = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
        "umap_base = reducer_base.fit_transform(df_active_scaled)\n",
        "\n",
        "# Add UMAP coordinates to our dataframe\n",
        "df_active['umap_base_1'] = umap_base[:, 0]\n",
        "df_active['umap_base_2'] = umap_base[:, 1]\n",
        "\n",
        "# 3. Plotting the Baseline Comparison (2D Feature Space vs. UMAP Manifold)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# Plot A: The Original \"Hype vs Utility\" Scatter (M3 Style)\n",
        "sns.scatterplot(\n",
        "    data=df_active,\n",
        "    x=df_active_log['github_repo_stars'],\n",
        "    y=df_active_log['number_of_jobs'],\n",
        "    hue='baseline_cluster',\n",
        "    palette='tab10',\n",
        "    alpha=0.6,\n",
        "    s=60,\n",
        "    ax=axes[0]\n",
        ")\n",
        "axes[0].set_title('M3 Baseline: Hype vs Utility Feature Space')\n",
        "axes[0].set_xlabel('Log10 GitHub Stars (Hype)')\n",
        "axes[0].set_ylabel('Log10 Number of Jobs (Utility)')\n",
        "axes[0].grid(True, linestyle=\"--\", alpha=0.5)\n",
        "\n",
        "# Plot B: The UMAP Manifold Projection\n",
        "sns.scatterplot(\n",
        "    data=df_active,\n",
        "    x='umap_base_1',\n",
        "    y='umap_base_2',\n",
        "    hue='baseline_cluster',\n",
        "    palette='tab10',\n",
        "    alpha=0.7,\n",
        "    ax=axes[1]\n",
        ")\n",
        "axes[1].set_title(\"M3 Baseline: UMAP Manifold Projection\")\n",
        "axes[1].set_xlabel(\"UMAP Dimension 1\")\n",
        "axes[1].set_ylabel(\"UMAP Dimension 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the numerical profiles for final baseline verification [cite: 55-59]\n",
        "print(\"Baseline Archetype Profiles (Mean values):\")\n",
        "display(df_active.groupby('baseline_cluster')[features].mean())"
      ],
      "metadata": {
        "id": "N1YUQsRkcUIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. M4 Method Upgrade: Autoencoder Feature Extraction**\n",
        "\n",
        "Traditional clustering relies on the \"surface\" geometry of the data. However, the programming language ecosystem is defined by non-linear relationships—for instance, a 10% increase in GitHub stars does not lead to a linear 10% increase in job postings.\n",
        "\n",
        "**The Refinement (Deep Learning Architecture):**\n",
        "We implement an **Autoencoder**, a type of neural network designed for unsupervised representation learning.\n",
        "\n",
        "* **The Squeeze:** The network compresses our 4D input features into a 2D or 4D **Latent Space** (the bottleneck).\n",
        "* **The Goal:** To reconstruct the original input, the network is forced to discard noise and multicollinear \"redundancy\" (like the overlap between Stars and Wiki views), capturing only the fundamental, non-linear \"DNA\" of the technology.\n",
        "* **The Benefit:** By clustering on this latent space rather than raw features, we can identify archetypes based on their structural positioning in the market rather than superficial volume."
      ],
      "metadata": {
        "id": "iy2xLGyteA8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 1. Define the Autoencoder Architecture\n",
        "# Input dimension matches our 4 features: Stars, Jobs, Users, Wiki Views\n",
        "input_dim = df_active_scaled.shape[1]\n",
        "encoding_dim = 4  # The \"Latent Space\" Bottleneck\n",
        "\n",
        "# Encoder: Compressing the signal\n",
        "input_layer = layers.Input(shape=(input_dim,))\n",
        "encoded = layers.Dense(16, activation='relu')(input_layer)\n",
        "bottleneck = layers.Dense(encoding_dim, activation='relu', name='latent_space')(encoded)\n",
        "\n",
        "# Decoder: Attempting to reconstruct the original input\n",
        "decoded = layers.Dense(16, activation='relu')(bottleneck)\n",
        "output_layer = layers.Dense(input_dim, activation='linear')(decoded)\n",
        "\n",
        "# 2. Build and Compile the Model\n",
        "autoencoder = models.Model(input_layer, output_layer)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# 3. Training the Model\n",
        "# The model learns by trying to predict its own input (X = y)\n",
        "print(\"Training Autoencoder to extract latent features...\")\n",
        "history = autoencoder.fit(\n",
        "    df_active_scaled,\n",
        "    df_active_scaled,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    verbose=0,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "# 4. Extracting the Latent \"DNA\"\n",
        "# We create a sub-model that stops at the bottleneck layer\n",
        "encoder_only = models.Model(inputs=autoencoder.input,\n",
        "                             outputs=autoencoder.get_layer('latent_space').output)\n",
        "\n",
        "latent_features = encoder_only.predict(df_active_scaled)\n",
        "\n",
        "# 5. Visualizing the Training Loss (Rigor Check)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Autoencoder Training Loss: Ensuring Convergence')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O_DXK9BmeEVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Advanced Clustering & UMAP Visualization**\n",
        "\n",
        "With the \"Latent DNA\" of the programming languages extracted, we now perform our final clustering. Unlike Section 4, which clustered based on surface-level metrics, this stage clusters based on the compressed, non-linear representations learned by the Autoencoder.\n",
        "\n",
        "**The Refinement Logic:**\n",
        "* **Algorithm:** K-Means ($k=4$) applied to the **Latent Space**.\n",
        "* **Visualization:** UMAP projection of the **Latent Space**.\n",
        "* **The Comparison:** By projecting the deep features into a 2D UMAP space, we can observe if the cluster boundaries are better defined and if the \"Adoption Chasm\" between Speculative languages and Industry Standards is more distinct."
      ],
      "metadata": {
        "id": "o1sD7zfEeVuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. New K-Means on the Deep (Latent) Features\n",
        "kmeans_deep = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "df_active['deep_cluster'] = kmeans_deep.fit_predict(latent_features)\n",
        "\n",
        "# 2. UMAP on the Deep Features (The Latent Manifold)\n",
        "reducer_deep = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
        "umap_deep = reducer_deep.fit_transform(latent_features)\n",
        "\n",
        "# Add Deep UMAP coordinates to our dataframe\n",
        "df_active['umap_deep_1'] = umap_deep[:, 0]\n",
        "df_active['umap_deep_2'] = umap_deep[:, 1]\n",
        "\n",
        "# 3. Final Comparison Plot: Baseline vs. Deep Archetypes\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# Plot A: Baseline UMAP (From Section 4)\n",
        "sns.scatterplot(\n",
        "    data=df_active, x='umap_base_1', y='umap_base_2',\n",
        "    hue='baseline_cluster', palette='tab10', alpha=0.5, ax=axes[0]\n",
        ")\n",
        "axes[0].set_title(\"M3 Baseline: UMAP on Raw Features\")\n",
        "axes[0].set_xlabel(\"UMAP Dimension 1\")\n",
        "axes[0].set_ylabel(\"UMAP Dimension 2\")\n",
        "\n",
        "# Plot B: Deep Archetype UMAP (The M4 Refinement)\n",
        "sns.scatterplot(\n",
        "    data=df_active, x='umap_deep_1', y='umap_deep_2',\n",
        "    hue='deep_cluster', palette='viridis', alpha=0.7, ax=axes[1]\n",
        ")\n",
        "axes[1].set_title(\"M4 Refined: UMAP on Autoencoder Latent Space\")\n",
        "axes[1].set_xlabel(\"UMAP Dimension 1\")\n",
        "axes[1].set_ylabel(\"UMAP Dimension 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Profile Check: How do these new deep clusters look?\n",
        "print(\"Deep Archetype Profiles (Mean values):\")\n",
        "display(df_active.groupby('deep_cluster')[features].mean().sort_values(by='number_of_jobs', ascending=False))"
      ],
      "metadata": {
        "id": "sSO7Rng3eVRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Business Insights: Identifying \"Archetype Shifts\"**\n",
        "\n",
        "The true value of our Deep Learning refinement is its ability to look past surface-level \"buzz\" and identify languages based on their underlying technological and industrial DNA. By comparing the Baseline (M3) clusters with our Refined (M4) Deep clusters, we can identify **\"Archetype Shifts.\"**\n",
        "\n",
        "**Strategic Value for CTOs and Investors:**\n",
        "* **Hidden Titans:** Languages that appear niche or speculative on the surface but possess the structural profile of an industry standard.\n",
        "* **False Hype Detection:** Technologies that command high social media \"stargazing\" but lack the foundational ecosystem support to survive long-term.\n",
        "* **The Adoption Chasm:** Quantifying how far a speculative language must evolve to become a \"Silent Workhorse\" or \"Mainstream Titan.\""
      ],
      "metadata": {
        "id": "RPTJ4JBteflv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Identify the \"Shifters\"\n",
        "# These are languages where the Deep DNA (Autoencoder) overrode the surface metrics\n",
        "df_active['shifted'] = df_active['baseline_cluster'] != df_active['deep_cluster']\n",
        "shifters = df_active[df_active['shifted'] == True].copy()\n",
        "\n",
        "print(f\"Total Languages Analyzed: {len(df_active)}\")\n",
        "print(f\"Number of Languages that Shifted Archetypes: {len(shifters)}\")\n",
        "\n",
        "# 2. Display a sample of \"Shifting\" languages for qualitative analysis\n",
        "# We look at the change from M3 logic to M4 deep logic\n",
        "print(\"\\n--- Sample of Archetype Shifters (M3 Baseline vs. M4 Deep DNA) ---\")\n",
        "display(shifters[['title', 'baseline_cluster', 'deep_cluster', 'github_repo_stars', 'number_of_jobs']].head(15))\n",
        "\n",
        "# 3. Final Strategic Mapping\n",
        "# Assigning the business names to our final Deep Clusters for clarity\n",
        "cluster_map = {\n",
        "    0: \"Silent Workhorses\",\n",
        "    1: \"Niche / Long-Tail\",\n",
        "    2: \"Speculative Disruptors\",\n",
        "    3: \"Mainstream Titans\"\n",
        "}\n",
        "# Note: Ensure the numbers match your actual cluster means from Section 6\n",
        "df_active['archetype_name'] = df_active['deep_cluster'].map(cluster_map)\n",
        "\n",
        "# 4. Save the Refined Results\n",
        "df_active.to_csv(\"M4_Refined_Market_Archetypes.csv\", index=False)\n",
        "print(\"\\n✅ Refinement Complete. Data saved to 'M4_Refined_Market_Archetypes.csv'\")"
      ],
      "metadata": {
        "id": "-1fjdTccefQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n"
      ],
      "metadata": {
        "id": "aNr7WC2peevX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Archetype Branding & Interpretation\n",
        "Cluster 2: The Market Titans (High Utility, High Hype)\n",
        "\n",
        "Members: SQL, Java, JavaScript, HTML, HTTP.\n",
        "\n",
        "Interpretation: These are the infrastructure of the modern web and data world. They have the highest job counts and massive social visibility. They represent the \"Safe Bets\" for any developer or firm.\n",
        "\n",
        "Cluster 0: The Modern Trendsetters (High Hype, Rising Utility)\n",
        "\n",
        "Members: Kotlin, TypeScript, Solidity, Elixir.\n",
        "\n",
        "Interpretation: These are \"New-Era\" languages. They have high GitHub engagement and are technically robust (captured by your Autoencoder). They are transitioning from \"trendy\" to \"essential,\" particularly in mobile (Kotlin), web (TS), and Web3 (Solidity).\n",
        "\n",
        "Cluster 1: Technical Specialists (Niche Utility)\n",
        "\n",
        "Members: Node.js, Liquid, k-framework.\n",
        "\n",
        "Interpretation: These represent specific ecosystems (e.g., Shopify’s Liquid or server-side JS). They aren't general-purpose \"Titans,\" but they have dedicated professional pockets and specific technical DNA that the Autoencoder recognized.\n",
        "\n",
        "Cluster 3: The Long-Tail / Speculative (Experimental)\n",
        "\n",
        "Members: Remix, Quaint-lang, Preforth.\n",
        "\n",
        "Interpretation: Even within the \"Active\" subset, these are languages with low professional demand but some social or technical activity. They represent the \"Experimental\" fringe of the ecosystem."
      ],
      "metadata": {
        "id": "4bmdYzRz703K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refined Problem Statement and Reflection\n",
        "\n",
        "Challenge: My original analysis was hindered by 97% zero-job data and linear models (PCA) that couldn't distinguish between \"Famous\" and \"Useful\" languages.\n",
        "\n",
        "Action: I narrowed the scope to 953 \"Active Market\" languages and implemented a Deep Learning Autoencoder to extract the \"Latent DNA\" of the ecosystem.\n",
        "\n",
        "Reflection: The Autoencoder revealed that technical structure is a major predictor of archetype. While Cluster 2 (Titans) is obvious, Cluster 0 (Modern Trendsetters) is the most valuable discovery. These languages (like Kotlin and Solidity) have the technical foundation of Titans but are still in their growth phase.\n",
        "\n",
        "Business Value: This model allows stakeholders to look past \"Hype\" and see which technologies have the technical robustness to provide long-term professional stability."
      ],
      "metadata": {
        "id": "ccGPN7jI75dz"
      }
    }
  ]
}
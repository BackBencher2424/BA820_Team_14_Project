{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BackBencher2424/BA820_Team_14_Project/blob/main/BA820_M3_Integrated_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d696f6d",
      "metadata": {
        "id": "9d696f6d"
      },
      "source": [
        "# BA820 — M3 Integrated Notebook (Best-of M2 EDA + Unified Preprocessing)\n",
        "\n",
        "**Team 14 | Phase M3**\n",
        "\n",
        "This notebook **reproduces the core concepts** from the four Phase M2 notebooks (Q1–Q4) while **combining the best EDA + preprocessing** into a single, consistent pipeline.  \n",
        "You can directly compare M3 results with M2 results because the notebook preserves the **same kinds of visuals** (distributions, log-scale plots, UMAP/PCA, elbow/silhouette curves, quadrant “ghost” view, survival-category plots, and cluster profiles).\n",
        "\n",
        "---\n",
        "\n",
        "## What we combined (and how)\n",
        "\n",
        "### Best EDA (combined from M2)\n",
        "- **Column standardization + missingness table** (from **M2 Q2/Q3**)  \n",
        "- **Top-missing bar chart** (from **M2 Q4**)  \n",
        "- **Skew-aware log-scale distributions** for stars and Wikipedia views + key scatterplots (from **M2 Q2/Q3**)  \n",
        "- **Duplicate check** using `pldb_id` (from **M2 Q3**)  \n",
        "- **Timeline sanity check** on `appeared` (from **M2 Q3**)  \n",
        "\n",
        "### Best Preprocessing (combined from M2)\n",
        "- **Robust numeric casting** (`pd.to_numeric(..., errors=\"coerce\")`) across key columns (from **M2 Q1/Q2/Q3**)  \n",
        "- **Skew handling** using `log1p` / `log10(x+1)` for community metrics (from **M2 Q1/Q2/Q3**)  \n",
        "- **Technical “extensions” parsing** and compact extension features (from **M2 Q1**)  \n",
        "- **Longevity features**: `age`, `years_since_last_activity`, and `survival_category` (from **M2 Q4**)  \n",
        "- **Ecosystem presence flags**: `has_domain`, `has_github`, `has_wikipedia` (from **M2 Q4**)  \n",
        "\n",
        "### How integration is enforced\n",
        "All analyses (Q1–Q4 reproduction + M3 integrated clustering) start from the **same** cleaned dataframe `df_clean`, and use analysis-specific “views”:\n",
        "- `df_comm` for community / adoption signals  \n",
        "- `df_tech` for technical/extension signals  \n",
        "- `df_hype` for hype-vs-utility archetypes + ghost-language scoring  \n",
        "- `df_long` for longevity / survival-based clustering  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8k2TaFV3z5j"
      },
      "source": [
        "## M3 Rubric Alignment (use these blocks in your M3 PDF)\n",
        "\n",
        "This notebook is structured to support **all M3 grading dimensions**:\n",
        "\n",
        "1) **Integrated Problem Framing & Updated Questions (8%)**  \n",
        "   - See: **Section 11 → 11.1** (table: unchanged / refined / dropped, with reasons tied to M2).\n",
        "\n",
        "2) **Recap of Individual M2 Contributions (5%)**  \n",
        "   - See: **Section 11 → 11.2** (concise member-by-member recap + strengths/limits).\n",
        "\n",
        "3) **Integration Strategy & Synergy Effort (20%)**  \n",
        "   - See: **Section 11 → 11.3** (integration log: reused / modified / discarded + why + what failed).\n",
        "\n",
        "4) **Integrated Analysis & Results (25%)**  \n",
        "   - See: **Section 10** (M3 integrated clustering + profiles) and **M3 vs M2 comparison heatmaps**.\n",
        "\n",
        "5) **Insights Gained Through Integration (20%)**  \n",
        "   - See: **Section 11 → 11.4–11.5** (reflection prompts + auto-summaries you can edit).\n",
        "\n",
        "6) **Limitations, Open Questions, & Next Steps (7%)**  \n",
        "   - See: **Section 11 → 11.6** (concrete, finding-linked next steps).\n",
        "\n",
        "7) **Documentation, Attribution & Compliance (8%)**  \n",
        "   - See: **Section 11 → 11.7** (repo link placeholder + contribution table + GenAI appendix template).\n",
        "\n",
        "> **Important:** The PDF is what gets graded, but the notebook provides the **evidence + copy/paste blocks** you need to score “Excellent.”\n"
      ],
      "id": "J8k2TaFV3z5j"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F5cMFUn3z5j"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 0) Setup (Colab-friendly)\n",
        "# =========================\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# UMAP is not always installed by default on Colab\n",
        "try:\n",
        "    import umap.umap_ as umap\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'umap-learn'])\n",
        "    import umap.umap_ as umap\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# ---------- Consistent style (high-contrast + readable) ----------\n",
        "# One palette for ALL categorical plots (clusters/archetypes/ghost status/etc.)\n",
        "PALETTE_NAME = \"colorblind\"                     # vivid + color-safe\n",
        "PALETTE = sns.color_palette(PALETTE_NAME, 10)   # stable ordering\n",
        "PRIMARY_COLOR = PALETTE[0]\n",
        "HEATMAP_CMAP = \"mako\"                           # blue-toned, higher contrast than pure \"Blues\"\n",
        "\n",
        "def blue_palette(n):\n",
        "    \"\"\"Return n distinct colors from the global palette (kept for backward compatibility).\"\"\"\n",
        "    n = int(max(3, n))\n",
        "    return sns.color_palette(PALETTE_NAME, n_colors=n)\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\", palette=PALETTE)\n",
        "sns.set_palette(PALETTE)\n",
        "\n",
        "# Smaller defaults -> cleaner notebook output in Colab\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": (9.2, 5.2),\n",
        "    \"figure.dpi\": 120,\n",
        "    \"axes.titlesize\": 13,\n",
        "    \"axes.labelsize\": 11,\n",
        "    \"legend.fontsize\": 10,\n",
        "    \"legend.title_fontsize\": 9,\n",
        "})\n",
        "\n",
        "# Keep tables from flooding output\n",
        "pd.set_option(\"display.max_rows\", 25)\n",
        "pd.set_option(\"display.max_columns\", 60)\n",
        "pd.set_option(\"display.width\", 120)\n",
        "\n",
        "print(\"✅ Setup complete (consistent palette + compact plot defaults enabled).\")\n"
      ],
      "id": "7F5cMFUn3z5j"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgNj-umr3z5k"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1) Load data (NO fetch)\n",
        "# =========================\n",
        "DATA_PATH = \"/content/languages.csv\"   # Upload languages.csv to Colab runtime.\n",
        "\n",
        "df_raw = pd.read_csv(DATA_PATH)\n",
        "df_raw.columns = [c.strip().lower() for c in df_raw.columns]\n",
        "\n",
        "# Drop rows where appeared < 1500 (project scope constraint)\n",
        "_rows_before = len(df_raw)\n",
        "if \"appeared\" in df_raw.columns:\n",
        "    df_raw[\"appeared\"] = pd.to_numeric(df_raw[\"appeared\"], errors=\"coerce\")\n",
        "    df_raw = df_raw[df_raw[\"appeared\"] >= 1500].copy()\n",
        "_rows_after = len(df_raw)\n"
      ],
      "id": "sgNj-umr3z5k"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fOE23Lj3z5k"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load summary (one output)\n",
        "summary = pd.DataFrame([{\n",
        "    \"data_path\": DATA_PATH,\n",
        "    \"rows_before_filter\": int(_rows_before),\n",
        "    \"rows_after_filter\": int(_rows_after),\n",
        "    \"rows_removed\": int(_rows_before - _rows_after)\n",
        "}])\n",
        "display(summary)\n"
      ],
      "id": "8fOE23Lj3z5k"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-CGfGg73z5k"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Preview (one output)\n",
        "display(df_raw.head(10))\n"
      ],
      "id": "L-CGfGg73z5k"
    },
    {
      "cell_type": "markdown",
      "id": "ea7a02c3",
      "metadata": {
        "id": "ea7a02c3"
      },
      "source": [
        "## 2) Best-of M2 EDA (Unified)\n",
        "\n",
        "This section **replaces repeated EDA** across M2 notebooks.  \n",
        "It keeps the *most grading-relevant* EDA visuals used in M2 so you can compare results consistently.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0aJJiJA3z5k"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.1 Basic schema + dtype overview (one output)\n",
        "display(df_raw.dtypes.value_counts().to_frame(\"count\"))\n"
      ],
      "id": "A0aJJiJA3z5k"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6jJBKyy3z5k"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.1 Preview (one output)\n",
        "display(df_raw.head(3))\n"
      ],
      "id": "w6jJBKyy3z5k"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZiUuAqv3z5l"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.2 Missingness summary (one output)\n",
        "missing = (df_raw.isna().sum()\n",
        "           .to_frame(\"missing_count\")\n",
        "           .assign(missing_pct=lambda x: (x[\"missing_count\"] / len(df_raw) * 100).round(2))\n",
        "           .sort_values(\"missing_pct\", ascending=False))\n",
        "\n",
        "display(missing.head(15))\n"
      ],
      "id": "9ZiUuAqv3z5l"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmbEInpO3z5l"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.2 Top missing columns (bar chart) — one output\n",
        "top10 = missing[missing[\"missing_count\"] > 0].head(10).sort_values(\"missing_pct\", ascending=True)\n",
        "\n",
        "plt.figure(figsize=(9.2, 5.2))\n",
        "plt.barh(top10.index.astype(str), top10[\"missing_pct\"].values, color=PRIMARY_COLOR, alpha=0.9)\n",
        "plt.title(\"Top columns by missing percentage\")\n",
        "plt.xlabel(\"Missing %\")\n",
        "plt.ylabel(\"Column\")\n",
        "\n",
        "for i_, v in enumerate(top10[\"missing_pct\"].values):\n",
        "    plt.text(v + 0.2, i_, f\"{v:.1f}%\", va=\"center\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "EmbEInpO3z5l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaaf3bad",
      "metadata": {
        "id": "aaaf3bad"
      },
      "outputs": [],
      "source": [
        "# 2.3 Duplicate check (M2 Q3 idea)\n",
        "if \"pldb_id\" in df_raw.columns:\n",
        "    dup_count = df_raw[\"pldb_id\"].duplicated().sum()\n",
        "    print(\"Duplicate pldb_id rows:\", dup_count)\n",
        "else:\n",
        "    print(\"pldb_id column not found — skipping duplicate-id check.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXsoZ0Or3z5l"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.4 Key distributions + log-scale views (prep; no output)\n",
        "def safe_numeric(s):\n",
        "    return pd.to_numeric(s, errors=\"coerce\")\n",
        "\n",
        "def log10p1(x):\n",
        "    x = pd.to_numeric(x, errors=\"coerce\")\n",
        "    return np.log10(x.clip(lower=0) + 1)\n",
        "\n",
        "core_cols = [\n",
        "    \"github_repo_stars\",\"wikipedia_daily_page_views\",\n",
        "    \"number_of_users\",\"number_of_jobs\",\n",
        "    \"github_repo_forks\",\"github_repo_subscribers\",\"github_repo_issues\",\n",
        "    \"github_language_repos\",\"wikipedia_backlinks_count\",\"wikipedia_revision_count\",\n",
        "    \"book_count\",\"central_package_repository_count\"\n",
        "]\n",
        "available_core = [c for c in core_cols if c in df_raw.columns]\n",
        "\n",
        "# Show the most important 6 metrics (same lens as M2)\n",
        "sel = available_core[:6]\n",
        "_cols_grid = 3\n",
        "_rows_grid = int(np.ceil(len(sel) / _cols_grid)) if len(sel) else 0\n"
      ],
      "id": "NXsoZ0Or3z5l"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l2LX6773z5l"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.4 Key metric distributions (raw) — one output\n",
        "if len(sel) > 0:\n",
        "    fig, axes = plt.subplots(_rows_grid, _cols_grid, figsize=(12.0, 6.0))\n",
        "    axes = np.array(axes).reshape(-1)\n",
        "\n",
        "    for ax, c in zip(axes, sel):\n",
        "        x = safe_numeric(df_raw[c]).dropna()\n",
        "        ax.hist(x, bins=30, color=PALETTE[1], alpha=0.9)\n",
        "        ax.set_title(f\"{c} (raw)\")\n",
        "        ax.set_xlabel(c)\n",
        "        ax.set_ylabel(\"Count\")\n",
        "\n",
        "    for ax in axes[len(sel):]:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(\"Key metric distributions (raw)\", y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    display(pd.DataFrame([{\"note\": \"No core numeric columns found for distributions.\"}]))\n"
      ],
      "id": "8l2LX6773z5l"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KYvNkvo3z5l"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.4 Key metric distributions (log10(x+1)) — one output\n",
        "if len(sel) > 0:\n",
        "    fig, axes = plt.subplots(_rows_grid, _cols_grid, figsize=(12.0, 6.0))\n",
        "    axes = np.array(axes).reshape(-1)\n",
        "\n",
        "    for ax, c in zip(axes, sel):\n",
        "        x = safe_numeric(df_raw[c]).dropna()\n",
        "        ax.hist(log10p1(x), bins=30, color=PALETTE[0], alpha=0.9)\n",
        "        ax.set_title(f\"{c} (log10(x+1))\")\n",
        "        ax.set_xlabel(f\"log10({c}+1)\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "\n",
        "    for ax in axes[len(sel):]:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(\"Key metric distributions (log10(x+1))\", y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "2KYvNkvo3z5l"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POwFtVDc3z5l"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.4 Key relationships (M2 lens) — one output\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12.0, 5.0))\n",
        "\n",
        "if {\"number_of_users\",\"number_of_jobs\"}.issubset(df_raw.columns):\n",
        "    tmp = df_raw[[\"number_of_users\",\"number_of_jobs\"]].apply(safe_numeric).dropna()\n",
        "    ax[0].scatter(log10p1(tmp[\"number_of_users\"]), log10p1(tmp[\"number_of_jobs\"]),\n",
        "                  s=14, alpha=0.6, color=PALETTE[2])\n",
        "    ax[0].set_xlabel(\"log10(users + 1)\")\n",
        "    ax[0].set_ylabel(\"log10(jobs + 1)\")\n",
        "    ax[0].set_title(\"Users vs Jobs (log)\")\n",
        "\n",
        "if {\"github_repo_stars\",\"wikipedia_daily_page_views\"}.issubset(df_raw.columns):\n",
        "    tmp = df_raw[[\"github_repo_stars\",\"wikipedia_daily_page_views\"]].apply(safe_numeric).dropna()\n",
        "    ax[1].scatter(log10p1(tmp[\"github_repo_stars\"]), log10p1(tmp[\"wikipedia_daily_page_views\"]),\n",
        "                  s=14, alpha=0.6, color=PALETTE[3])\n",
        "    ax[1].set_xlabel(\"log10(stars + 1)\")\n",
        "    ax[1].set_ylabel(\"log10(wiki views + 1)\")\n",
        "    ax[1].set_title(\"Stars vs Wiki views (log)\")\n",
        "\n",
        "plt.suptitle(\"Key relationships (M2 lens)\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "POwFtVDc3z5l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a547a16",
      "metadata": {
        "id": "2a547a16"
      },
      "outputs": [],
      "source": [
        "# 2.5 Timeline sanity check (M2 Q3)\n",
        "if \"appeared\" in df_raw.columns:\n",
        "    appeared = pd.to_numeric(df_raw[\"appeared\"], errors=\"coerce\")\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "    plt.hist(appeared.dropna(), bins=60, color=PALETTE[4], alpha=0.85)\n",
        "    plt.title(\"Timeline sanity check: 'appeared' year distribution\")\n",
        "    plt.xlabel(\"appeared (year)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No 'appeared' column found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bd1e5c",
      "metadata": {
        "id": "75bd1e5c"
      },
      "source": [
        "## 3) Unified Preprocessing (Best-of M2)\n",
        "\n",
        "Goal: create `df_clean` + analysis-ready views while keeping M2 concepts intact:\n",
        "- consistent numeric casting  \n",
        "- consistent log transforms for skewed metrics  \n",
        "- extension parsing (technical signals)  \n",
        "- longevity / survival features  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a7fd3c",
      "metadata": {
        "id": "81a7fd3c"
      },
      "outputs": [],
      "source": [
        "# 3.1 Start with a clean working copy\n",
        "df_clean = df_raw.copy()\n",
        "\n",
        "# 3.2 Numeric casting (common across M2 Q1/Q2/Q3/Q4)\n",
        "numeric_cols = [\n",
        "    \"appeared\",\"last_activity\",\"language_rank\",\n",
        "    \"github_repo_stars\",\"github_repo_forks\",\"github_repo_subscribers\",\"github_repo_issues\",\n",
        "    \"github_language_repos\",\n",
        "    \"wikipedia_daily_page_views\",\"wikipedia_backlinks_count\",\"wikipedia_revision_count\",\n",
        "    \"book_count\",\"central_package_repository_count\",\n",
        "    \"number_of_users\",\"number_of_jobs\",\n",
        "    \"ecosystem_score\"\n",
        "]\n",
        "for c in numeric_cols:\n",
        "    if c in df_clean.columns:\n",
        "        df_clean[c] = pd.to_numeric(df_clean[c], errors=\"coerce\")\n",
        "\n",
        "# 3.3 Date casting (Q2/Q3)\n",
        "date_cols = [\"github_repo_updated\",\"github_repo_created\",\"github_repo_first_commit\",\"wikipedia_created\"]\n",
        "for c in date_cols:\n",
        "    if c in df_clean.columns:\n",
        "        df_clean[c] = pd.to_datetime(df_clean[c], errors=\"coerce\")\n",
        "\n",
        "# 3.4 Boolean casting for language feature flags (Q4)\n",
        "bool_cols = [\"features_has_comments\",\"features_has_semantic_indentation\",\"features_has_line_comments\"]\n",
        "for c in bool_cols:\n",
        "    if c in df_clean.columns:\n",
        "        df_clean[c] = df_clean[c].astype(\"boolean\")\n",
        "\n",
        "# 3.5 Treat negative placeholders as missing, then clip counts at 0\n",
        "if \"wikipedia_daily_page_views\" in df_clean.columns:\n",
        "    df_clean.loc[df_clean[\"wikipedia_daily_page_views\"] < 0, \"wikipedia_daily_page_views\"] = np.nan\n",
        "\n",
        "count_like = [\n",
        "    \"github_repo_stars\",\"github_repo_forks\",\"github_repo_subscribers\",\"github_repo_issues\",\n",
        "    \"github_language_repos\",\n",
        "    \"wikipedia_daily_page_views\",\"wikipedia_backlinks_count\",\"wikipedia_revision_count\",\n",
        "    \"book_count\",\"central_package_repository_count\",\n",
        "    \"number_of_users\",\"number_of_jobs\",\n",
        "]\n",
        "for c in count_like:\n",
        "    if c in df_clean.columns:\n",
        "        df_clean[c] = df_clean[c].clip(lower=0)\n",
        "\n",
        "# 3.6 Timeline filter (M2 Q3): remove very early \"appeared\" years for comparability\n",
        "if \"appeared\" in df_clean.columns:\n",
        "    df_clean = df_clean[(df_clean[\"appeared\"].isna()) | (df_clean[\"appeared\"] >= 1500)].copy()\n",
        "\n",
        "# 3.7 Longevity features (M2 Q4)\n",
        "REF_YEAR = 2023\n",
        "if \"appeared\" in df_clean.columns:\n",
        "    df_clean[\"age\"] = REF_YEAR - df_clean[\"appeared\"]\n",
        "if \"last_activity\" in df_clean.columns:\n",
        "    df_clean[\"years_since_last_activity\"] = REF_YEAR - df_clean[\"last_activity\"]\n",
        "\n",
        "if \"years_since_last_activity\" in df_clean.columns:\n",
        "    bins = [-np.inf, 5, 15, np.inf]\n",
        "    labels = [\"Active\", \"Maintained\", \"Dormant\"]\n",
        "    df_clean[\"survival_category\"] = pd.cut(\n",
        "        df_clean[\"years_since_last_activity\"], bins=bins, labels=labels, right=False\n",
        "    )\n",
        "\n",
        "# 3.8 Ecosystem presence flags (M2 Q4)\n",
        "for col, newcol in [(\"domain_name\",\"has_domain\"), (\"github_repo\",\"has_github\"), (\"wikipedia\",\"has_wikipedia\")]:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[newcol] = df_clean[col].notna().astype(int)\n",
        "\n",
        "# 3.9 Log features (Q1/Q2/Q3/Q4)\n",
        "def add_log_features(df_in, col, kind=\"log1p\"):\n",
        "    if col not in df_in.columns:\n",
        "        return\n",
        "    x = pd.to_numeric(df_in[col], errors=\"coerce\").fillna(0).clip(lower=0)\n",
        "    if kind == \"log1p\":\n",
        "        df_in[f\"log1p_{col}\"] = np.log1p(x)\n",
        "    elif kind == \"log10\":\n",
        "        df_in[f\"log10_{col}\"] = np.log10(x + 1)\n",
        "\n",
        "for c in [\"number_of_users\",\"number_of_jobs\",\"wikipedia_daily_page_views\",\"github_repo_stars\",\n",
        "          \"github_repo_forks\",\"github_repo_subscribers\",\"wikipedia_backlinks_count\",\"wikipedia_revision_count\",\n",
        "          \"book_count\",\"central_package_repository_count\"]:\n",
        "    add_log_features(df_clean, c, \"log1p\")\n",
        "    add_log_features(df_clean, c, \"log10\")\n",
        "\n",
        "df_clean.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "767162d8",
      "metadata": {
        "id": "767162d8"
      },
      "source": [
        "### 3.10 Technical signal preprocessing: parse GitHub language extensions (M2 Q1)\n",
        "\n",
        "Creates:\n",
        "- `num_extensions_listed`\n",
        "- top-K extension one-hot columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d54fee8",
      "metadata": {
        "id": "7d54fee8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def parse_extensions(ext_str):\n",
        "    # Parse space-separated extensions into cleaned tokens.\n",
        "    if pd.isna(ext_str) or not isinstance(ext_str, str):\n",
        "        return []\n",
        "    tokens = ext_str.strip().lower().split()\n",
        "    cleaned=[]\n",
        "    for t in tokens:\n",
        "        t=t.strip()\n",
        "        if not t:\n",
        "            continue\n",
        "        t = re.sub(r\"[^a-z0-9\\.\\_\\-]+\",\"\",t)\n",
        "        if t:\n",
        "            cleaned.append(t)\n",
        "    return cleaned\n",
        "\n",
        "def sanitize_token(token):\n",
        "    return \"ext_\" + re.sub(r\"[^a-z0-9]+\", \"_\", token.lower()).strip(\"_\")\n",
        "\n",
        "if \"github_language_file_extensions\" in df_clean.columns:\n",
        "    ext_tokens = df_clean[\"github_language_file_extensions\"].apply(parse_extensions)\n",
        "    df_clean[\"num_extensions_listed\"] = ext_tokens.apply(lambda x: len(set(x)))\n",
        "\n",
        "    K = 30\n",
        "    all_ext = ext_tokens.explode()\n",
        "    top_ext = all_ext.value_counts().head(K).index.tolist()\n",
        "    for e in top_ext:\n",
        "        df_clean[sanitize_token(e)] = ext_tokens.apply(lambda xs: int(e in set(xs)))\n",
        "\n",
        "    print(f\"✅ Extension features added: num_extensions_listed + {len(top_ext)} dummies\")\n",
        "else:\n",
        "    print(\"github_language_file_extensions not found — skipping extension feature engineering.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0937e3ca",
      "metadata": {
        "id": "0937e3ca"
      },
      "source": [
        "## 4) Analysis-ready views (shared inputs for Q1–Q4 + M3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMwGOhQt3z5m"
      },
      "source": [
        "# 4.1 Build analysis-ready views from df_clean (required by Q1–Q4 + M3)\n",
        "# NOTE: these are *views* (subsets) — df_clean remains the single source of truth.\n",
        "\n",
        "# Community/adoption view (numeric)\n",
        "comm_candidates = [\n",
        "    'log10_wikipedia_daily_page_views','log10_github_repo_stars','log10_number_of_users','log10_number_of_jobs',\n",
        "    'log10_github_repo_forks','log10_github_repo_subscribers',\n",
        "    'log10_wikipedia_backlinks_count','log10_wikipedia_revision_count',\n",
        "    'log10_book_count','log10_central_package_repository_count',\n",
        "    'language_rank','ecosystem_score'\n",
        "]\n",
        "comm_cols = [c for c in comm_candidates if c in df_clean.columns]\n",
        "df_comm = df_clean[comm_cols].copy() if len(comm_cols) else pd.DataFrame(index=df_clean.index)\n",
        "\n",
        "# Technical/extensions view (numeric)\n",
        "tech_cols = []\n",
        "if 'num_extensions_listed' in df_clean.columns:\n",
        "    tech_cols.append('num_extensions_listed')\n",
        "tech_cols += [c for c in df_clean.columns if c.startswith('ext_')]\n",
        "df_tech = df_clean[tech_cols].copy() if len(tech_cols) else pd.DataFrame(index=df_clean.index)\n",
        "\n",
        "# Hype vs utility view (full row context; numeric subsets are selected per question)\n",
        "df_hype = df_clean.copy()\n",
        "\n",
        "# Longevity/ecosystem view (full row context; Q4 does encoding inside its section)\n",
        "df_long = df_clean.copy()\n"
      ],
      "id": "bMwGOhQt3z5m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nWynAKa3z5m"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Feature-view tables created for Q1–Q4 (one output)\n",
        "shapes = pd.DataFrame([\n",
        "    {\"view\": \"df_comm (community/adoption)\", \"rows\": df_comm.shape[0], \"cols\": df_comm.shape[1]},\n",
        "    {\"view\": \"df_tech (technical/extensions)\", \"rows\": df_tech.shape[0], \"cols\": df_tech.shape[1]},\n",
        "    {\"view\": \"df_hype (hype vs utility)\", \"rows\": df_hype.shape[0], \"cols\": df_hype.shape[1]},\n",
        "    {\"view\": \"df_long (longevity/ecosystem)\", \"rows\": df_long.shape[0], \"cols\": df_long.shape[1]},\n",
        "])\n",
        "display(shapes)\n"
      ],
      "id": "0nWynAKa3z5m"
    },
    {
      "cell_type": "markdown",
      "id": "578e5b82",
      "metadata": {
        "id": "578e5b82"
      },
      "source": [
        "## 5) Shared utilities (silhouette sweep, PCA/UMAP plots)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ_uvItL3z5m"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def build_numeric_pipeline():\n",
        "    return Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "    ])\n",
        "\n",
        "pipe = build_numeric_pipeline()\n",
        "\n",
        "def silhouette_sweep_kmeans(X, k_list):\n",
        "    rows=[]\n",
        "    for k in k_list:\n",
        "        model = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
        "        labels = model.fit_predict(X)\n",
        "        s = silhouette_score(X, labels) if len(set(labels)) > 1 else np.nan\n",
        "        rows.append({\"k\":k, \"silhouette\":s, \"inertia\":model.inertia_})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def plot_elbow_sil(df_scores, title_prefix=\"\"):\n",
        "    \"\"\"Compact elbow + silhouette plot (avoids huge outputs).\"\"\"\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12.0, 5.0))\n",
        "    ax[0].plot(df_scores[\"k\"], df_scores[\"inertia\"], marker=\"o\")\n",
        "    ax[0].set_title(f\"{title_prefix} Elbow (Inertia)\")\n",
        "    ax[0].set_xlabel(\"k\")\n",
        "    ax[0].set_ylabel(\"Inertia (SSE)\")\n",
        "\n",
        "    ax[1].plot(df_scores[\"k\"], df_scores[\"silhouette\"], marker=\"o\")\n",
        "    ax[1].set_title(f\"{title_prefix} Silhouette vs k\")\n",
        "    ax[1].set_xlabel(\"k\")\n",
        "    ax[1].set_ylabel(\"Silhouette\")\n",
        "    plt.tight_layout(rect=[0, 0, 0.82, 1])\n",
        "    plt.show()\n",
        "\n",
        "def _cluster_color_map(labels):\n",
        "    uniq = sorted(pd.unique(pd.Series(labels).dropna()))\n",
        "    color_map = {u: PALETTE[i % len(PALETTE)] for i, u in enumerate(uniq)}\n",
        "    return uniq, color_map\n",
        "\n",
        "def scatter_clusters(Z, labels, title, xlabel, ylabel, legend_title=\"Cluster\"):\n",
        "    labels = np.asarray(labels)\n",
        "    uniq, cmap = _cluster_color_map(labels)\n",
        "\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "    for u in uniq:\n",
        "        m = labels == u\n",
        "        plt.scatter(Z[m,0], Z[m,1], s=14, alpha=0.65, color=cmap[u], label=str(u))\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend(title=legend_title, bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "    plt.tight_layout(rect=[0, 0, 0.82, 1])\n",
        "    plt.show()\n",
        "\n",
        "def pca_scatter(X, labels, title=\"PCA scatter\"):\n",
        "    pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
        "    Z = pca.fit_transform(X)\n",
        "    scatter_clusters(Z, labels, title=title, xlabel=\"PC1\", ylabel=\"PC2\", legend_title=\"Cluster\")\n",
        "    return pca, Z\n",
        "\n",
        "def umap_scatter(X, labels, title=\"UMAP scatter\", n_neighbors=15, min_dist=0.1):\n",
        "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, random_state=RANDOM_STATE)\n",
        "    Z = reducer.fit_transform(X)\n",
        "    scatter_clusters(Z, labels, title=title, xlabel=\"UMAP-1\", ylabel=\"UMAP-2\", legend_title=\"Cluster\")\n",
        "    return reducer, Z\n",
        "\n",
        "k_list = list(range(2, 11))\n"
      ],
      "id": "DJ_uvItL3z5m"
    },
    {
      "cell_type": "markdown",
      "id": "b47e2b9b",
      "metadata": {
        "id": "b47e2b9b"
      },
      "source": [
        "## 6) Reproduce M2 Q1 (Vishesh): Technical vs Community clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "126ae80e",
      "metadata": {
        "id": "126ae80e"
      },
      "outputs": [],
      "source": [
        "# 6.1 Q1 Technical clustering (extensions)\n",
        "tech_num = [c for c in df_tech.columns if c not in [\"appeared\",\"last_activity\"]]\n",
        "X_tech = df_tech[tech_num].copy()\n",
        "X_tech_prep = pipe.fit_transform(X_tech)\n",
        "\n",
        "scores_tech = silhouette_sweep_kmeans(X_tech_prep, k_list)\n",
        "display(scores_tech)\n",
        "plot_elbow_sil(scores_tech, title_prefix=\"Q1 Technical\")\n",
        "\n",
        "best_k_tech = int(scores_tech.sort_values(\"silhouette\", ascending=False).iloc[0][\"k\"])\n",
        "labels_tech = KMeans(n_clusters=best_k_tech, random_state=RANDOM_STATE, n_init=10).fit_predict(X_tech_prep)\n",
        "\n",
        "pca_scatter(X_tech_prep, labels_tech, title=f\"Q1 Technical PCA (k={best_k_tech})\")\n",
        "umap_scatter(X_tech_prep, labels_tech, title=f\"Q1 Technical UMAP (k={best_k_tech})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d644571",
      "metadata": {
        "id": "3d644571"
      },
      "outputs": [],
      "source": [
        "# 6.2 Q1 Community clustering\n",
        "X_comm = df_comm.copy()\n",
        "X_comm_prep = pipe.fit_transform(X_comm)\n",
        "\n",
        "scores_comm = silhouette_sweep_kmeans(X_comm_prep, k_list)\n",
        "display(scores_comm)\n",
        "plot_elbow_sil(scores_comm, title_prefix=\"Q1 Community\")\n",
        "\n",
        "best_k_comm = int(scores_comm.sort_values(\"silhouette\", ascending=False).iloc[0][\"k\"])\n",
        "labels_comm = KMeans(n_clusters=best_k_comm, random_state=RANDOM_STATE, n_init=10).fit_predict(X_comm_prep)\n",
        "\n",
        "pca_scatter(X_comm_prep, labels_comm, title=f\"Q1 Community PCA (k={best_k_comm})\")\n",
        "umap_scatter(X_comm_prep, labels_comm, title=f\"Q1 Community UMAP (k={best_k_comm})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa35faaa",
      "metadata": {
        "id": "fa35faaa"
      },
      "source": [
        "## 7) Reproduce M2 Q2 (Drishti): Hype vs Utility archetypes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs0tFIcg3z5n"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Q2 (M2 Drishti): Hype vs Utility archetypes\n",
        "# -------------------------\n",
        "q2_features = [\"log10_github_repo_stars\",\"log10_wikipedia_daily_page_views\",\"log10_number_of_users\",\"log10_number_of_jobs\"]\n",
        "q2_features = [c for c in q2_features if c in df_hype.columns]\n",
        "assert len(q2_features) >= 3, f\"Not enough Q2 features found. Available: {df_hype.columns.tolist()}\"\n",
        "\n",
        "X_q2 = df_hype[q2_features].copy()\n",
        "X_q2_prep = pipe.fit_transform(X_q2)\n",
        "\n",
        "# Model selection metrics (kept from M2; visuals shown in next cells)\n",
        "scores_q2 = silhouette_sweep_kmeans(X_q2_prep, k_list)\n"
      ],
      "id": "Qs0tFIcg3z5n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O_o7r9T3z5n"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q2: silhouette + inertia table (one output)\n",
        "display(scores_q2)\n"
      ],
      "id": "6O_o7r9T3z5n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgfCM9Ud3z5n"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q2: elbow + silhouette plot (one output)\n",
        "plot_elbow_sil(scores_q2, title_prefix=\"Q2 Hype vs Utility\")\n"
      ],
      "id": "pgfCM9Ud3z5n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqfWoheE3z5n"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q2: archetype assignment (k=4) + counts (one output)\n",
        "k_q2 = 4  # preserve M2 baseline choice\n",
        "df_hype[\"q2_cluster\"] = KMeans(n_clusters=k_q2, random_state=RANDOM_STATE, n_init=10).fit_predict(X_q2_prep)\n",
        "\n",
        "q2_counts = df_hype[\"q2_cluster\"].value_counts().sort_index().to_frame(\"count\")\n",
        "display(q2_counts)\n"
      ],
      "id": "lqfWoheE3z5n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHcX5WBy3z5n"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q2 archetype visualization (keep the same lens as M2: stars vs jobs)\n",
        "if {\"log10_github_repo_stars\",\"log10_number_of_jobs\"}.issubset(df_hype.columns):\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "    sns.scatterplot(\n",
        "        data=df_hype,\n",
        "        x=\"log10_github_repo_stars\",\n",
        "        y=\"log10_number_of_jobs\",\n",
        "        hue=\"q2_cluster\",\n",
        "        palette=blue_palette(df_hype[\"q2_cluster\"].nunique()),\n",
        "        s=20,\n",
        "        alpha=0.65,\n",
        "        edgecolor=\"white\", linewidth=0.3\n",
        "    )\n",
        "    plt.xlabel(\"log10(stars + 1)\")\n",
        "    plt.ylabel(\"log10(jobs + 1)\")\n",
        "    plt.title(\"Q2 Archetypes: Hype (stars) vs Utility (jobs)\")\n",
        "    plt.legend(title=\"Q2 archetype\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "    plt.tight_layout(rect=[0, 0, 0.82, 1])\n",
        "    plt.show()\n"
      ],
      "id": "SHcX5WBy3z5n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf_DVWyl3z5n"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# NEW BAR GRAPH (Q2): Archetype Distribution\n",
        "counts = df_hype[\"q2_cluster\"].value_counts().sort_index()\n",
        "plt.figure(figsize=(9.2, 5.2))\n",
        "sns.barplot(\n",
        "    x=counts.index.astype(str),\n",
        "    y=counts.values,\n",
        "    palette=blue_palette(len(counts)), edgecolor=\"black\", linewidth=0.3\n",
        ")\n",
        "plt.title(\"Q2 Archetype Distribution (count)\")\n",
        "plt.xlabel(\"Q2 archetype (cluster id)\")\n",
        "plt.ylabel(\"Number of languages\")\n",
        "for i_, v in enumerate(counts.values):\n",
        "    plt.text(i_, v, str(int(v)), ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "Zf_DVWyl3z5n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM9OduGu3z5n"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q2 archetype profiles (same as M2: mean feature values by archetype)\n",
        "profile_q2 = df_hype.groupby(\"q2_cluster\")[q2_features].mean().round(3).sort_index()\n",
        "display(profile_q2)\n"
      ],
      "id": "KM9OduGu3z5n"
    },
    {
      "cell_type": "markdown",
      "id": "d2b1f31c",
      "metadata": {
        "id": "d2b1f31c"
      },
      "source": [
        "## 8) Reproduce M2 Q3 (Arshdeep): Ghost languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridBMP_s3z5n"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Q3 (M2 Arshdeep): Ghost languages (high visibility, low employment)\n",
        "# -------------------------\n",
        "vis_cols = [c for c in [\"log10_github_repo_stars\",\"log10_wikipedia_daily_page_views\",\"log10_github_repo_subscribers\"] if c in df_hype.columns]\n",
        "emp_cols = [c for c in [\"log10_number_of_jobs\",\"log10_number_of_users\"] if c in df_hype.columns]\n",
        "\n",
        "assert len(vis_cols) >= 2, f\"Need at least 2 visibility cols. Found: {vis_cols}\"\n",
        "assert len(emp_cols) >= 1, f\"Need at least 1 employment col. Found: {emp_cols}\"\n",
        "\n",
        "df_hype[\"visibility_score\"] = df_hype[vis_cols].mean(axis=1)\n",
        "df_hype[\"employment_score\"] = df_hype[emp_cols].mean(axis=1)\n",
        "\n",
        "x = df_hype[\"visibility_score\"]\n",
        "y = df_hype[\"employment_score\"]\n",
        "x_thr = x.mean()\n",
        "y_thr = y.median()\n",
        "\n",
        "plt.figure(figsize=(9.2, 5.2))\n",
        "plt.scatter(x, y, s=12, alpha=0.6, color=PALETTE[2], label=\"Language\")\n",
        "plt.axvline(x_thr, linestyle=\"--\", color=PALETTE[0], label=\"Mean visibility\")\n",
        "plt.axhline(y_thr, linestyle=\"--\", color=PALETTE[1], label=\"Median employment\")\n",
        "plt.xlabel(\"Visibility score (avg log metrics)\")\n",
        "plt.ylabel(\"Employment score (avg log metrics)\")\n",
        "plt.title(\"Q3 Ghost-language quadrant (high visibility, low employment)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "ridBMP_s3z5n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzVClXM_3z5s"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Define ghost status (same rule as M2 Q3) + NEW BAR GRAPH (Q3): Ghost vs Non-Ghost\n",
        "df_hype[\"ghost_quadrant\"] = np.where((x > x_thr) & (y < y_thr), 1, 0)\n",
        "\n",
        "ghost_counts = df_hype[\"ghost_quadrant\"].value_counts().reindex([0,1]).fillna(0).astype(int)\n",
        "labels = [\"Non-Ghost\", \"Ghost\"]\n",
        "vals = [ghost_counts.get(0,0), ghost_counts.get(1,0)]\n",
        "\n",
        "plt.figure(figsize=(9.2, 5.2))\n",
        "sns.barplot(x=labels, y=vals, palette=blue_palette(2), edgecolor=\"black\", linewidth=0.3)\n",
        "plt.title(\"Q3: Ghost vs Non-Ghost (count)\")\n",
        "plt.xlabel(\"Status\")\n",
        "plt.ylabel(\"Number of languages\")\n",
        "for i_, v in enumerate(vals):\n",
        "    plt.text(i_, v, str(int(v)), ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "XzVClXM_3z5s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYEJq6Su3z5s"
      },
      "source": [
        "### 8.1 Q3 Clustering (K-Means + Silhouette) with PCA + UMAP (M2-comparable)\n",
        "\n",
        "We reproduce the **M2 Q3 clustering idea** (cluster languages using visibility + adoption + activity signals),\n",
        "but run it on the unified `df_hype` view so the results are directly comparable with Q2 and the final M3 synergy.\n",
        "\n",
        "**Key constraints (as requested):** K-Means + Silhouette only (no elbow), plus **PCA** and **UMAP** visualizations.\n"
      ],
      "id": "ZYEJq6Su3z5s"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-UgvKD03z5s"
      },
      "source": [
        "# Q3 clustering features (same spirit as M2_Q3): visibility + popularity + adoption signals\n",
        "# Prefer raw columns if present; otherwise fall back to unified log1p versions (keeps the concept intact).\n",
        "q3_feat_map = [\n",
        "    (\"wikipedia_daily_page_views\", \"log1p_wikipedia_daily_page_views\"),\n",
        "    (\"github_repo_stars\", \"log1p_github_repo_stars\"),\n",
        "    (\"github_repo_forks\", \"log1p_github_repo_forks\"),\n",
        "    (\"github_language_repos\", \"github_language_repos\"),\n",
        "    (\"language_rank\", \"language_rank\"),\n",
        "    (\"number_of_jobs\", \"log1p_number_of_jobs\"),\n",
        "    (\"number_of_users\", \"log1p_number_of_users\"),\n",
        "]\n",
        "\n",
        "q3_features = []\n",
        "for raw, alt in q3_feat_map:\n",
        "    if raw in df_hype.columns:\n",
        "        q3_features.append(raw)\n",
        "    elif alt in df_hype.columns:\n",
        "        q3_features.append(alt)\n",
        "\n",
        "if len(q3_features) < 4:\n",
        "    raise ValueError(f\"Not enough Q3 clustering features found. Found: {q3_features}\")\n",
        "\n",
        "q3_feature_df = df_hype[q3_features].copy()\n",
        "# M2 used fillna(0) then scaled\n",
        "q3_feature_df = q3_feature_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "X_q3 = StandardScaler().fit_transform(q3_feature_df)\n"
      ],
      "id": "l-UgvKD03z5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXk0d9IG3z5s"
      },
      "source": [
        "# Silhouette sweep (K-Means only) — one output table\n",
        "k_list = list(range(2, 8))\n",
        "rows = []\n",
        "for k in k_list:\n",
        "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
        "    lab = km.fit_predict(X_q3)\n",
        "    s = silhouette_score(X_q3, lab)\n",
        "    rows.append({\"k\": k, \"silhouette\": round(float(s), 4)})\n",
        "\n",
        "q3_sil = pd.DataFrame(rows)\n",
        "display(q3_sil)\n"
      ],
      "id": "UXk0d9IG3z5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04AOXHDn3z5s"
      },
      "source": [
        "# Silhouette plot (choose k with best silhouette) — one output\n",
        "best_k_q3 = int(q3_sil.sort_values('silhouette', ascending=False).iloc[0]['k'])\n",
        "\n",
        "plt.figure(figsize=(9.2, 5.2))\n",
        "plt.plot(q3_sil['k'], q3_sil['silhouette'], marker='o', color=PRIMARY_COLOR)\n",
        "plt.axvline(best_k_q3, linestyle='--', color=PALETTE[1], label=f\"best k = {best_k_q3}\")\n",
        "plt.title('Q3 Silhouette Scores (K-Means)')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "plt.tight_layout(rect=[0, 0, 0.82, 1])\n",
        "plt.show()\n"
      ],
      "id": "04AOXHDn3z5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmyusWPe3z5s"
      },
      "source": [
        "# Fit final Q3 K-Means + show cluster size distribution (bar) — one output\n",
        "kmeans_q3 = KMeans(n_clusters=best_k_q3, random_state=RANDOM_STATE, n_init=10)\n",
        "df_hype['q3_cluster'] = kmeans_q3.fit_predict(X_q3)\n",
        "\n",
        "counts = pd.Series(df_hype['q3_cluster']).value_counts().sort_index()\n",
        "plt.figure(figsize=(9.2, 5.2))\n",
        "sns.barplot(x=counts.index.astype(str), y=counts.values, palette=blue_palette(len(counts)), edgecolor='black', linewidth=0.3)\n",
        "plt.title(f'Q3 Cluster Sizes (k={best_k_q3})')\n",
        "plt.xlabel('Q3 cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "jmyusWPe3z5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsFEz4he3z5s"
      },
      "source": [
        "# Q3 PCA scatter — one output\n",
        "pca_scatter(X_q3, df_hype['q3_cluster'], title=f\"Q3 PCA (k={best_k_q3})\")\n"
      ],
      "id": "jsFEz4he3z5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WURtIc413z5s"
      },
      "source": [
        "# Q3 UMAP scatter — one output\n",
        "umap_scatter(X_q3, df_hype['q3_cluster'], title=f\"Q3 UMAP (k={best_k_q3})\")\n"
      ],
      "id": "WURtIc413z5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0zzwV8S3z5s"
      },
      "source": [
        "# M2-style view: Visibility vs Employment colored by Q3 clusters, highlighting Ghost languages — one output\n",
        "plt.figure(figsize=(9.2, 5.2))\n",
        "sns.scatterplot(x=df_hype['visibility_score'], y=df_hype['employment_score'], hue=df_hype['q3_cluster'],\n",
        "                palette=blue_palette(df_hype['q3_cluster'].nunique()), s=24, alpha=0.65, edgecolor=None)\n",
        "\n",
        "# highlight ghosts\n",
        "ghost_df = df_hype[df_hype['ghost_quadrant'] == 1]\n",
        "plt.scatter(ghost_df['visibility_score'], ghost_df['employment_score'],\n",
        "            s=55, facecolors='none', edgecolors=PALETTE[2], linewidths=1.2, label='Ghost (outline)')\n",
        "\n",
        "plt.axvline(x_thr, linestyle='--', color=PALETTE[0])\n",
        "plt.axhline(y_thr, linestyle='--', color=PALETTE[1])\n",
        "plt.title('Q3 Visibility vs Employment (colored by Q3 clusters)')\n",
        "plt.xlabel('Visibility score')\n",
        "plt.ylabel('Employment score')\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "plt.tight_layout(rect=[0, 0, 0.80, 1])\n",
        "plt.show()\n"
      ],
      "id": "Q0zzwV8S3z5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6exJ_G73z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q3: Ghost quadrant summary (one output)\n",
        "ghost_count = int(df_hype[\"ghost_quadrant\"].sum()) if \"ghost_quadrant\" in df_hype.columns else 0\n",
        "ghost_summary = pd.DataFrame([{\n",
        "    \"ghost_quadrant_count\": ghost_count,\n",
        "    \"total_rows_in_df_hype\": int(len(df_hype)),\n",
        "    \"ghost_pct\": round(ghost_count / max(1, len(df_hype)) * 100, 2)\n",
        "}])\n",
        "display(ghost_summary)\n"
      ],
      "id": "l6exJ_G73z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVd-dQDb3z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q3: Example ghost languages (one output)\n",
        "if \"title\" in df_hype.columns and {\"visibility_score\",\"employment_score\",\"ghost_quadrant\"}.issubset(df_hype.columns):\n",
        "    ex = (df_hype.loc[df_hype[\"ghost_quadrant\"]==1, [\"title\",\"visibility_score\",\"employment_score\"]]\n",
        "          .sort_values(\"visibility_score\", ascending=False)\n",
        "          .head(15))\n",
        "    display(ex)\n",
        "else:\n",
        "    display(pd.DataFrame([{\"note\": \"Required columns not found for ghost examples.\"}]))\n"
      ],
      "id": "CVd-dQDb3z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bWSVz893z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q3 additional visual (M2 Q3 style): pairplot (sampled + compact)\n",
        "pair_cols = []\n",
        "for c in [\"log10_github_repo_stars\",\"log10_wikipedia_daily_page_views\",\"log10_github_repo_subscribers\",\n",
        "          \"log10_number_of_jobs\",\"log10_number_of_users\",\"visibility_score\",\"employment_score\"]:\n",
        "    if c in df_hype.columns:\n",
        "        pair_cols.append(c)\n",
        "\n",
        "pair_df = df_hype[pair_cols].dropna()\n",
        "\n",
        "# downsample for speed\n",
        "if len(pair_df) > 800:\n",
        "    pair_df = pair_df.sample(800, random_state=RANDOM_STATE)\n",
        "\n",
        "if len(pair_cols) >= 4 and len(pair_df) > 50:\n",
        "    sns.pairplot(pair_df, corner=True, plot_kws={\"s\": 10, \"alpha\": 0.35}, height=1.4)\n",
        "    plt.suptitle(\"Pairplot of key visibility/employment features (sampled)\", y=1.02)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Not enough columns/rows for pairplot; skipping.\")\n"
      ],
      "id": "6bWSVz893z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1AMqS4q3z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q3 additional visual: correlation heatmap (compact + blue palette)\n",
        "if len(pair_cols) >= 3 and len(pair_df) > 50:\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "    sns.heatmap(\n",
        "        pair_df.corr(numeric_only=True),\n",
        "        cmap=HEATMAP_CMAP,\n",
        "        annot=True,\n",
        "        fmt=\".2f\",\n",
        "        cbar_kws={\"label\":\"Correlation\"}\n",
        "    )\n",
        "    plt.title(\"Correlation heatmap (key ghost-language features)\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "j1AMqS4q3z5t"
    },
    {
      "cell_type": "markdown",
      "id": "b4014dc9",
      "metadata": {
        "id": "b4014dc9"
      },
      "source": [
        "## 9) Reproduce M2 Q4 (Ahrar): Longevity / survival clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg7wB4hF3z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Q4 (M2 Ahrar): Longevity / survival clustering\n",
        "# -------------------------\n",
        "# Survival category distribution (bar)\n",
        "if \"survival_category\" in df_clean.columns:\n",
        "    counts = df_clean[\"survival_category\"].value_counts()\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "    sns.barplot(\n",
        "        x=counts.index.astype(str),\n",
        "        y=counts.values,\n",
        "        palette=blue_palette(len(counts))\n",
        "    )\n",
        "    plt.title(\"Q4: Distribution of survival categories\")\n",
        "    plt.xlabel(\"survival_category\")\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.xticks(rotation=20, ha=\"right\")\n",
        "    for i_, v in enumerate(counts.values):\n",
        "        plt.text(i_, v, str(int(v)), ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "wg7wB4hF3z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8F64Tzn3z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Survival category distribution (pie) — optional but kept for M2 comparability (blue shades)\n",
        "if \"survival_category\" in df_clean.columns:\n",
        "    counts = df_clean[\"survival_category\"].value_counts()\n",
        "    plt.figure(figsize=(7.0, 7.0))\n",
        "    plt.pie(\n",
        "        (counts / counts.sum()).values,\n",
        "        labels=counts.index.astype(str),\n",
        "        autopct=\"%1.1f%%\",\n",
        "        startangle=90,\n",
        "        colors=blue_palette(len(counts))\n",
        "    )\n",
        "    plt.title(\"Q4: Survival category percentages\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "X8F64Tzn3z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CVRugCS3z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q4 feature set + clustering evaluation (silhouette + elbow)\n",
        "selected_features = [\n",
        "    \"type\",\"file_type\",\"features_has_comments\",\"features_has_semantic_indentation\",\"features_has_line_comments\",\n",
        "    \"has_domain\",\"has_github\",\"has_wikipedia\",\n",
        "    \"ecosystem_score\",\"age\",\"years_since_last_activity\",\n",
        "    \"log1p_number_of_users\",\"log1p_number_of_jobs\",\"log1p_book_count\",\"log1p_central_package_repository_count\"\n",
        "]\n",
        "selected_features = [c for c in selected_features if c in df_long.columns]\n",
        "df_q4 = df_long[selected_features].copy()\n",
        "\n",
        "categorical_cols = [c for c in [\"type\",\"file_type\",\"features_has_comments\",\"features_has_semantic_indentation\",\"features_has_line_comments\"] if c in df_q4.columns]\n",
        "df_q4_enc = pd.get_dummies(df_q4, columns=categorical_cols, dummy_na=True)\n",
        "\n",
        "X_q4 = df_q4_enc.fillna(0)\n",
        "X_q4_prep = pipe.fit_transform(X_q4)\n",
        "\n",
        "scores_q4 = silhouette_sweep_kmeans(X_q4_prep, k_list)\n",
        "display(scores_q4)\n",
        "plot_elbow_sil(scores_q4, title_prefix=\"Q4 Longevity\")\n",
        "\n",
        "best_k_q4 = int(scores_q4.sort_values(\"silhouette\", ascending=False).iloc[0][\"k\"])\n",
        "labels_q4 = KMeans(n_clusters=best_k_q4, random_state=RANDOM_STATE, n_init=10).fit_predict(X_q4_prep)\n"
      ],
      "id": "8CVRugCS3z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_QcHDPo3z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q4 embeddings (PCA + UMAP)\n",
        "pca_scatter(X_q4_prep, labels_q4, title=f\"Q4 PCA (k={best_k_q4})\")\n",
        "umap_scatter(X_q4_prep, labels_q4, title=f\"Q4 UMAP (k={best_k_q4})\")\n"
      ],
      "id": "K_QcHDPo3z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf_iTvPv3z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Q4 cluster mix vs survival_category (table)\n",
        "df_tmp = df_clean.copy()\n",
        "df_tmp[\"q4_cluster\"] = labels_q4\n",
        "if \"survival_category\" in df_tmp.columns:\n",
        "    mix = pd.crosstab(df_tmp[\"q4_cluster\"], df_tmp[\"survival_category\"], normalize=\"index\").round(3)\n",
        "    display(mix)\n"
      ],
      "id": "Hf_iTvPv3z5t"
    },
    {
      "cell_type": "markdown",
      "id": "218a17bb",
      "metadata": {
        "id": "218a17bb"
      },
      "source": [
        "## 10) M3 Integrated Clustering (Synergy result)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1LO98BM3z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# M3 Integrated Clustering (Synergy result)\n",
        "# -------------------------\n",
        "integrated_cols = []\n",
        "\n",
        "integrated_cols += [c for c in [\n",
        "    \"log1p_github_repo_stars\",\"log1p_wikipedia_daily_page_views\",\n",
        "    \"log1p_number_of_users\",\"log1p_number_of_jobs\",\n",
        "    \"log1p_github_repo_subscribers\",\"log1p_github_repo_forks\"\n",
        "] if c in df_clean.columns]\n",
        "\n",
        "integrated_cols += [c for c in [\"age\",\"years_since_last_activity\",\"ecosystem_score\"] if c in df_clean.columns]\n",
        "integrated_cols += [c for c in [\"has_domain\",\"has_github\",\"has_wikipedia\"] if c in df_clean.columns]\n",
        "integrated_cols += [c for c in [\"num_extensions_listed\"] if c in df_clean.columns]\n",
        "\n",
        "integrated_cols = list(dict.fromkeys(integrated_cols))\n",
        "\n",
        "X_m3 = df_clean[integrated_cols].copy()\n",
        "X_m3_prep = pipe.fit_transform(X_m3)\n",
        "\n",
        "scores_m3 = silhouette_sweep_kmeans(X_m3_prep, k_list)\n"
      ],
      "id": "i1LO98BM3z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jblCKJVm3z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# M3 integrated feature set (one output)\n",
        "display(pd.DataFrame({\"integrated_feature\": integrated_cols}))\n"
      ],
      "id": "jblCKJVm3z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTC3atEY3z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# M3: silhouette + inertia table (one output)\n",
        "display(scores_m3)\n"
      ],
      "id": "FTC3atEY3z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVv94-N83z5t"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# M3: elbow + silhouette plot (one output)\n",
        "plot_elbow_sil(scores_m3, title_prefix=\"M3 Integrated\")\n"
      ],
      "id": "RVv94-N83z5t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98_xSJDo3z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# M3: chosen k (best silhouette) — one output\n",
        "best_k_m3 = int(scores_m3.sort_values(\"silhouette\", ascending=False).iloc[0][\"k\"])\n",
        "display(pd.DataFrame([{\"chosen_k\": best_k_m3}]))\n"
      ],
      "id": "98_xSJDo3z5u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPt__vl_3z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Fit final M3 clustering (prep; no output)\n",
        "df_clean[\"m3_cluster\"] = KMeans(n_clusters=best_k_m3, random_state=RANDOM_STATE, n_init=10).fit_predict(X_m3_prep)\n"
      ],
      "id": "yPt__vl_3z5u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCP3pzDo3z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# M3 embedding (PCA) — one output\n",
        "pca_scatter(X_m3_prep, df_clean[\"m3_cluster\"], title=f\"M3 Integrated PCA (k={best_k_m3})\")\n"
      ],
      "id": "BCP3pzDo3z5u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq0EB1LR3z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# M3 embedding (UMAP) — one output\n",
        "umap_scatter(X_m3_prep, df_clean[\"m3_cluster\"], title=f\"M3 Integrated UMAP (k={best_k_m3})\")\n"
      ],
      "id": "Mq0EB1LR3z5u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLEsgAGa3z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cluster profiles (mean feature values)\n",
        "profile_m3 = df_clean.groupby(\"m3_cluster\")[integrated_cols].mean().round(3).sort_index()\n",
        "display(profile_m3)\n"
      ],
      "id": "WLEsgAGa3z5u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQGpXGtM3z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Bring in Q2 archetypes + Q3 ghost flag for synergy comparisons\n",
        "# (Keeps original M2 definitions; we only merge them into one table.)\n",
        "if \"q2_cluster\" in df_hype.columns:\n",
        "    df_clean.loc[df_hype.index, \"q2_cluster\"] = df_hype[\"q2_cluster\"]\n",
        "if \"ghost_quadrant\" in df_hype.columns:\n",
        "    df_clean.loc[df_hype.index, \"ghost_quadrant\"] = df_hype[\"ghost_quadrant\"]\n",
        "\n",
        "# Human-readable ghost status\n",
        "if \"ghost_quadrant\" in df_clean.columns:\n",
        "    df_clean[\"ghost_status\"] = df_clean[\"ghost_quadrant\"].map({0:\"Non-Ghost\", 1:\"Ghost\"})\n",
        "\n",
        "# Crosstabs (used below in annotated heatmaps)\n",
        "ctab = None\n",
        "ctab2 = None\n",
        "if \"q2_cluster\" in df_clean.columns:\n",
        "    ctab = pd.crosstab(df_clean[\"m3_cluster\"], df_clean[\"q2_cluster\"], normalize=\"index\").round(3)\n",
        "\n",
        "if \"survival_category\" in df_clean.columns:\n",
        "    ctab2 = pd.crosstab(df_clean[\"m3_cluster\"], df_clean[\"survival_category\"], normalize=\"index\").round(3)\n",
        "\n",
        "# Compact comparison figure (keeps notebook clean)\n",
        "n_plots = int(ctab is not None) + int(ctab2 is not None)\n",
        "if n_plots > 0:\n",
        "    fig_w = 14 if n_plots == 2 else 10\n",
        "    fig, axes = plt.subplots(1, n_plots, figsize=(fig_w, 5.5))\n",
        "    if n_plots == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    ax_i = 0\n",
        "    if ctab is not None:\n",
        "        sns.heatmap(ctab, annot=True, fmt=\".2f\", cmap=HEATMAP_CMAP,\n",
        "                    cbar_kws={\"label\":\"Row proportion\"}, ax=axes[ax_i])\n",
        "        axes[ax_i].set_title(\"M3 clusters vs Q2 archetypes\")\n",
        "        axes[ax_i].set_xlabel(\"Q2 archetype\")\n",
        "        axes[ax_i].set_ylabel(\"M3 cluster\")\n",
        "        axes[ax_i].tick_params(axis=\"x\", rotation=30)\n",
        "        axes[ax_i].tick_params(axis=\"y\", rotation=0)\n",
        "        axes[ax_i].tick_params(axis=\"x\", rotation=30)\n",
        "        axes[ax_i].tick_params(axis=\"y\", rotation=0)\n",
        "        ax_i += 1\n",
        "\n",
        "    if ctab2 is not None:\n",
        "        sns.heatmap(ctab2, annot=True, fmt=\".2f\", cmap=HEATMAP_CMAP,\n",
        "                    cbar_kws={\"label\":\"Row proportion\"}, ax=axes[ax_i])\n",
        "        axes[ax_i].set_title(\"M3 clusters vs Survival categories\")\n",
        "        axes[ax_i].set_xlabel(\"survival_category\")\n",
        "        axes[ax_i].set_ylabel(\"M3 cluster\")\n",
        "        axes[ax_i].tick_params(axis=\"x\", rotation=30)\n",
        "        axes[ax_i].tick_params(axis=\"y\", rotation=0)\n",
        "        axes[ax_i].tick_params(axis=\"x\", rotation=30)\n",
        "        axes[ax_i].tick_params(axis=\"y\", rotation=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Missing required columns for synergy crosstabs; skipping.\")\n"
      ],
      "id": "ZQGpXGtM3z5u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQuky88I3z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# NEW BAR GRAPH (M3 synergy): Archetype vs Ghost Status\n",
        "if {\"q2_cluster\",\"ghost_status\"}.issubset(df_clean.columns):\n",
        "    tmp = df_clean.dropna(subset=[\"q2_cluster\",\"ghost_status\"]).copy()\n",
        "    tmp[\"q2_cluster\"] = tmp[\"q2_cluster\"].astype(int).astype(str)\n",
        "\n",
        "    ghost_status_order = [\"Non-Ghost\", \"Ghost\"]\n",
        "    ghost_status_pal = {k: PALETTE[i % len(PALETTE)] for i, k in enumerate(ghost_status_order)}\n",
        "\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "    sns.countplot(\n",
        "        data=tmp,\n",
        "        x=\"q2_cluster\",\n",
        "        hue=\"ghost_status\",\n",
        "        hue_order=ghost_status_order,\n",
        "        palette=ghost_status_pal\n",
        "    )\n",
        "    plt.title(\"M3 Synergy: Q2 Archetype vs Ghost Status (count)\")\n",
        "    plt.xlabel(\"Q2 archetype (cluster id)\")\n",
        "    plt.ylabel(\"Number of languages\")\n",
        "    plt.legend(title=\"Ghost status\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "    plt.tight_layout(rect=[0, 0, 0.82, 1])\n",
        "    plt.show()\n"
      ],
      "id": "JQuky88I3z5u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvcUMQY53z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# NEW BAR GRAPH (M3 synergy): Archetype vs Survival\n",
        "if {\"q2_cluster\",\"survival_category\"}.issubset(df_clean.columns):\n",
        "    tmp = df_clean.dropna(subset=[\"q2_cluster\",\"survival_category\"]).copy()\n",
        "    tmp[\"q2_cluster\"] = tmp[\"q2_cluster\"].astype(int).astype(str)\n",
        "\n",
        "    survival_order = sorted(tmp[\"survival_category\"].unique())\n",
        "    survival_pal = {k: PALETTE[i % len(PALETTE)] for i, k in enumerate(survival_order)}\n",
        "\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "    sns.countplot(\n",
        "        data=tmp,\n",
        "        x=\"q2_cluster\",\n",
        "        hue=\"survival_category\",\n",
        "        hue_order=survival_order,\n",
        "        palette=survival_pal\n",
        "    )\n",
        "    plt.title(\"M3 Synergy: Q2 Archetype vs Survival Category (count)\")\n",
        "    plt.xlabel(\"Q2 archetype (cluster id)\")\n",
        "    plt.ylabel(\"Number of languages\")\n",
        "    plt.legend(title=\"survival_category\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "    plt.tight_layout(rect=[0, 0, 0.82, 1])\n",
        "    plt.show()\n"
      ],
      "id": "DvcUMQY53z5u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmQmpCJk3z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# NEW BAR GRAPH (M3 synergy): Ghost languages — Archetype vs Survival\n",
        "if {\"q2_cluster\",\"survival_category\",\"ghost_status\"}.issubset(df_clean.columns):\n",
        "    tmp = df_clean.dropna(subset=[\"q2_cluster\",\"survival_category\",\"ghost_status\"]).copy()\n",
        "    tmp = tmp[tmp[\"ghost_status\"]==\"Ghost\"]\n",
        "    if len(tmp) > 0:\n",
        "        tmp[\"q2_cluster\"] = tmp[\"q2_cluster\"].astype(int).astype(str)\n",
        "\n",
        "        survival_order = sorted(tmp[\"survival_category\"].unique())\n",
        "        survival_pal = {k: PALETTE[i % len(PALETTE)] for i, k in enumerate(survival_order)}\n",
        "\n",
        "        plt.figure(figsize=(9.2, 5.2))\n",
        "        sns.countplot(\n",
        "            data=tmp,\n",
        "            x=\"q2_cluster\",\n",
        "            hue=\"survival_category\",\n",
        "            hue_order=survival_order,\n",
        "            palette=survival_pal\n",
        "        )\n",
        "        plt.title(\"M3 Synergy: Ghost Languages — Q2 Archetype vs Survival (count)\")\n",
        "        plt.xlabel(\"Q2 archetype (cluster id)\")\n",
        "        plt.ylabel(\"Number of ghost languages\")\n",
        "        plt.legend(title=\"survival_category\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "        plt.tight_layout(rect=[0, 0, 0.82, 1])\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No ghost languages found after filtering; skipping plot.\")\n"
      ],
      "id": "vmQmpCJk3z5u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP4aB8Uj3z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# NEW BAR GRAPH (M3 synergy): One Q1 feature vs one Q2 feature\n",
        "# We choose a numeric Q1 feature (prefer technical signal) and compare against a Q2 hype signal.\n",
        "\n",
        "q1_candidates = [\"num_extensions_listed\", \"github_language_repos\", \"wikipedia_backlinks_count\"]\n",
        "q2_candidates = [\"log10_github_repo_stars\", \"log1p_github_repo_stars\", \"github_repo_stars\"]\n",
        "\n",
        "q1_feat = next((c for c in q1_candidates if c in df_clean.columns), None)\n",
        "q2_feat = next((c for c in q2_candidates if c in df_clean.columns), None)\n",
        "\n",
        "if q1_feat and q2_feat:\n",
        "    tmp = df_clean[[q1_feat, q2_feat]].dropna().copy()\n",
        "\n",
        "    # Bin the Q1 feature to make a clean bar chart\n",
        "    if tmp[q1_feat].nunique() > 10:\n",
        "        tmp[\"q1_bin\"] = pd.qcut(tmp[q1_feat], q=4, duplicates=\"drop\")\n",
        "    else:\n",
        "        tmp[\"q1_bin\"] = tmp[q1_feat].astype(str)\n",
        "\n",
        "    agg = tmp.groupby(\"q1_bin\")[q2_feat].mean().reset_index()\n",
        "\n",
        "    plt.figure(figsize=(9.2, 5.2))\n",
        "    sns.barplot(data=agg, x=\"q1_bin\", y=q2_feat, color=PRIMARY_COLOR, edgecolor=\"black\", linewidth=0.4)\n",
        "    plt.title(f\"M3 Synergy: Mean {q2_feat} across {q1_feat} bins\")\n",
        "    plt.xlabel(f\"{q1_feat} (binned)\")\n",
        "    plt.ylabel(f\"Mean {q2_feat}\")\n",
        "    plt.xticks(rotation=25, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Could not find required features for Q1-vs-Q2 bar plot. Found:\", q1_feat, q2_feat)\n"
      ],
      "id": "JP4aB8Uj3z5u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKyb8ol73z5u"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Examples per M3 cluster (single table; one output)\n",
        "if \"title\" in df_clean.columns and \"m3_cluster\" in df_clean.columns:\n",
        "    ex = df_clean.copy()\n",
        "    score_cols = [c for c in [\"github_repo_stars\",\"number_of_jobs\",\"wikipedia_daily_page_views\"] if c in ex.columns]\n",
        "    if score_cols:\n",
        "        ex[\"_score\"] = ex[score_cols].fillna(0).sum(axis=1)\n",
        "    else:\n",
        "        ex[\"_score\"] = 0\n",
        "\n",
        "    ex = ex.sort_values([\"m3_cluster\",\"_score\"], ascending=[True, False])\n",
        "    top_examples = ex.groupby(\"m3_cluster\").head(10)[[\"m3_cluster\",\"title\"] + score_cols].reset_index(drop=True)\n",
        "    display(top_examples)\n",
        "else:\n",
        "    display(pd.DataFrame([{\"note\": \"Required columns not found for cluster examples.\"}]))\n"
      ],
      "id": "TKyb8ol73z5u"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}